#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
stereo â†’ ReazonSpeechâ€‘ESPnet ASR â†’ WhisperX alignment ã€ˆ**wholeâ€‘file mode**ã€‰
==============================================================================
ðŸ†• å„ãƒãƒ£ãƒãƒ«ã‚’ **ä¸¸ã”ã¨ä¸€æ‹¬æŽ¨è«–** ã™ã‚‹ã“ã¨ã§æ–‡è„ˆåˆ‡ã‚Œã‚’é˜²æ­¢ã—ã¾ã™ã€‚
* châ€‘0 = Speakerâ€¯A, châ€‘1 = Speakerâ€¯B
* 8â€¯kHz Î¼â€‘law â†’ 16â€¯kHz PCM ã¸è‡ªå‹•ãƒªã‚µãƒ³ãƒ—ãƒ«
* WhisperX ã§ wordâ€‘level æ•´åˆ—ã—ã€A+B ã‚’æ™‚ç³»åˆ—ãƒžãƒ¼ã‚¸
* æƒ³å®š vRAM: â‰ˆ2.0â€¯GB / 15â€¯min / ch (fp16) â€” GPU ã«ä½™è£•ãŒã‚ã‚‹å ´åˆã®ã¿åˆ©ç”¨

ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€  / ä½¿ã„æ–¹ã¯å¤‰æ›´ãªã—ã€‚
"""

from __future__ import annotations

import argparse
import json
import multiprocessing as mp
import os
import sys
import traceback
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
from typing import List

import torch
import torchaudio
from tqdm import tqdm

# â”€â”€â”€ Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ROOT = Path("/home/acg17145sv/experiments/0162_dialogue_model/data_stage_3/Tabidachi")
IN_ROOT = ROOT / "audio"
TXT_ROOT = ROOT / "transcripts"
ALN_ROOT = ROOT / "text"
for p in (TXT_ROOT, ALN_ROOT):
    p.mkdir(parents=True, exist_ok=True)

SPEAKERS = ("A", "B")
SAMPLE_RATE = 16_000

# â”€â”€â”€ Helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def resample_2ch(wav: torch.Tensor, sr: int) -> torch.Tensor:
    """Return (2, T) @16â€¯kHz CPU tensor, duplicating mono if needed."""
    if sr != SAMPLE_RATE:
        wav = torchaudio.functional.resample(wav, sr, SAMPLE_RATE)
    if wav.ndim == 1:
        wav = wav.unsqueeze(0).repeat(2, 1)
    elif wav.shape[0] == 1:
        wav = wav.repeat(2, 1)
    return wav.cpu()


# â”€â”€â”€ Worker â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def worker(device: str, wav_paths: List[Path], align_threads: int = 2):
    torch.cuda.set_device(device)
    os.environ["OMP_NUM_THREADS"] = "1"

    from reazonspeech.espnet.asr import load_model, transcribe, audio_from_numpy
    import whisperx

    asr_model = load_model(device=device)
    align_model, meta = whisperx.load_align_model("ja", device)
    align_exec = ThreadPoolExecutor(max_workers=align_threads)

    tag = os.getenv("PBS_ARRAY_INDEX") or os.getenv("SLURM_ARRAY_TASK_ID") or "solo"
    logfile = Path(f"Tabidachi_align_errors.log").open("a", encoding="utf-8")

    def elog(msg: str):
        print(msg, flush=True)
        logfile.write(msg + "\n")
        logfile.flush()

    for wav in tqdm(wav_paths, desc=f"[GPU {device}]"):
        try:
            wav_tensor, sr = torchaudio.load(wav)
            wav_tensor = resample_2ch(wav_tensor, sr)
            rel = wav.relative_to(IN_ROOT)
            txt_dir = TXT_ROOT / rel.parent
            txt_dir.mkdir(parents=True, exist_ok=True)

            segs_per_spk: list[list[dict]] = []

            # â”€â”€ ASR: whole file per channel â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            for ch, spk in enumerate(SPEAKERS):
                t_json = txt_dir / f"{wav.stem}_{spk}.json"
                if t_json.exists():
                    segs_per_spk.append(json.load(t_json.open())["segments"])
                    continue

                ret = transcribe(
                    asr_model, audio_from_numpy(wav_tensor[ch].numpy(), SAMPLE_RATE)
                )
                segments = [
                    {
                        "start": seg.start_seconds,
                        "end": seg.end_seconds,
                        "text": seg.text,
                    }
                    for seg in ret.segments
                ]

                json.dump(
                    {"text": ret.text, "segments": segments},
                    t_json.open("w", encoding="utf-8"),
                    ensure_ascii=False,
                    indent=2,
                )
                segs_per_spk.append(segments)

            # â”€â”€ WhisperX alignment â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            futures = [
                align_exec.submit(
                    whisperx.align,
                    segs_per_spk[idx],
                    align_model,
                    meta,
                    wav_tensor[idx].numpy(),
                    device,
                    False,
                    # interpolate_method="linear",  # æœ¬å½“ã¯interpolate_method ã®ã‚¨ãƒ©ãƒ¼ã‚’ä¿®æ­£ã™ã‚‹ãŸã‚ã«ï¼Œã“ã‚Œã‚’ã¤ã‘ãŸã„ãŒï¼Œãã†ã™ã‚‹ã¨åˆ¥ã®ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ï¼Œå‡¦ç†ãƒŸã‚¹ã‚’ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¢—ãˆã‚‹ã®ã§ï¼Œã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã—ã¦ãŠãï¼Ž
                    # return_char_alignments=False,
                    # print_progress=False,
                    # combined_progress=False,
                )
                for idx in range(2)
            ]

            merged = []
            for idx, fut in enumerate(futures):
                aligned = fut.result()
                spk = SPEAKERS[idx]
                merged.extend(
                    {
                        "speaker": spk,
                        "word": w["word"],
                        "start": w["start"],
                        "end": w["end"],
                    }
                    for seg in aligned["segments"]
                    for w in seg["words"]
                )

            merged.sort(key=lambda x: x["start"])
            out = ALN_ROOT / rel.parent / f"{wav.stem}.json"
            out.parent.mkdir(parents=True, exist_ok=True)
            json.dump(
                merged, out.open("w", encoding="utf-8"), ensure_ascii=False, indent=2
            )

        except Exception:
            elog(f"ERROR processing {wav}")
            traceback.print_exc(file=logfile)
            continue


# â”€â”€â”€ Utility â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def is_done(wav: Path) -> bool:
    p = ALN_ROOT / f"{wav.stem}.json"
    return p.is_file() and p.stat().st_size > 0


# â”€â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if __name__ == "__main__":
    mp.set_start_method("spawn", force=True)

    ap = argparse.ArgumentParser()
    ap.add_argument("--dirs", nargs="+", default=["."], help="subâ€‘dirs under audio/")
    args = ap.parse_args()

    targets: list[Path] = []
    for d in args.dirs:
        sub = IN_ROOT / d
        if not sub.is_dir():
            print(f"[warn] {sub} not found", file=sys.stderr)
            continue
        targets.extend(p for p in sub.rglob("*.wav") if not is_done(p))

    print(f"{len(targets)} wav files to process.")
    if not targets:
        sys.exit(0)

    gpus = [f"cuda:{i}" for i in range(torch.cuda.device_count())]
    chunks = [targets[i :: len(gpus)] for i in range(len(gpus))] if gpus else [targets]

    procs = [
        mp.Process(target=worker, args=(dev, chunk), daemon=False)
        for dev, chunk in zip(gpus, chunks)
    ]
    for p in procs:
        p.start()
    for p in procs:
        p.join()

    print("=== Tabidachi processing complete ===")
